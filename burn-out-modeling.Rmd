---
title: "Burn Out Modeling"
output: html_notebook
---

```{r include=FALSE,echo=FALSE,warning=FALSE}
library(tidyverse)
library(tidymodels)

knitr::opts_chunk$set(echo=FALSE,include=FALSE,warning = FALSE)
theme_set(ggthemes::theme_hc())

burn_out <- read_csv("/Users/rashawnhoward/Downloads/train.csv",col_names = TRUE) %>% drop_na() %>% janitor::clean_names()
```

```{r}
burn_out %>% 
  select(-employee_id) %>% 
  mutate(day = lubridate::day(date_of_joining),
         month = lubridate::month(date_of_joining),
         year = lubridate::year(date_of_joining),
         gender = as.factor(gender),
         company_type = as.factor(company_type),
         wfh_setup_available = as.factor(wfh_setup_available),
         designation = as.factor(designation),
         resource_allocation = as.factor(resource_allocation)) %>% 
  select(-date_of_joining) -> burn_out
```

```{r}
set.seed(8442)
splits <- initial_split(burn_out, prop = .8, strata = burn_rate)
train <- training(splits)
```

# Metrics and Controls
The metrics used to investigate the models are RMSE, R-squared, CCC, and MAE.
```{r}
metrics <- metric_set(rmse,rsq,mae,yardstick::ccc)
ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
```

# Resamples
10-fold cross-validation is used to split the data for tuning and evaluation of training models.
```{r}
set.seed(2028)
folds <- vfold_cv(train, v = 10)
```

# Linear Models
OLS, ridge, LASSO, elastic net regression was used on the data. The tuning parameters for the penalty models was found using 10-fold cross-validation over a random grid of 20 values.
### PreProcess
The numeric predictors were transformed using Yeojohnson transformations, centered and scaled, dummy variables were made for the nominal predictors, near-zero
variance filter was done on the predictor's space, and a correlation filter was done on the predictors.
```{r}
lin_rec <- recipe(burn_rate~., data = train) %>% 
  step_rm(year,day,month) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = .70)
```

```{r}
lm_mod <- linear_reg(
  engine = "lm")

ridge_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = 0)

lasso_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = 1)

elastic_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = tune())

linear_mods_list <- list(lm_mod,ridge_mod,lasso_mod,elastic_mod)
```

```{r}
lm_wf <- workflow_set(list(lin_rec),models = linear_mods_list)

lm_wf <- workflow_map(
  lm_wf,
  "tune_grid",
  grid = 20,
  verbose = TRUE,
  metrics = metrics,
  control = ctrl,
  resamples = folds
  )
```

```{r}
lm_wf %>% autoplot(metric = "rmse")
lm_wf %>% rank_results(rank_metric = "rmse")
```

# Non-Linear Models

### KNN
KNN was tuned over the number of neighbors (k) in the range 1-10 inclusive.

### MARS
MARS model was tuned over the degree of polynomial 1 or 2 and the number of terms to keep in the model.

### NNET
The single-layer neural network model tuned over the number of hidden units [1,10], dropout, weight decay, and 500 training iterations.

### SVM
A radial basis support vector machine tuned over cost, sigma, and margin with 20 randomly spaced values.

### Preprocess 
The preprocessing of the data for the non-linear models included centering and scaling numeric predictors, making dummy variables for all nominal predictors, and checking for near-zero variance predictors.
```{r}
nlin_rec <- recipe(burn_rate~.,data = burn_out) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())
```

```{r}
knn_spec <- nearest_neighbor( 
  mode = "regression",
  engine = "kknn",
  neighbors = tune()
  )

mars_spec <- mars(
  mode = "regression",
  engine = "earth",
  num_terms = tune(),
  prod_degree = tune()
  )
nnet_spec <- mlp(mode = "regression",
                 engine = "nnet",
                 hidden_units = tune(),
                 penalty = tune(),
                 dropout = tune(),
                 epochs = 500,
                 activation = "linear"
                 )
svm_spec <- svm_rbf(mode = "regression",
                    engine = "kernlab",
                    cost = tune(),
                    rbf_sigma = tune(),
                    margin = tune()
                    )
non_linear_models <- list(knn_spec,mars_spec,nnet_spec,svm_spec)
```

```{r}
nlm_wf <- workflow_set(list(nlin_rec),models = non_linear_models)

nlm_wf <- workflow_map(
  nlm_wf,
  "tune_grid",
  verbose = TRUE,
  grid = 20,
  metrics = metrics,
  control = ctrl,
  resamples = folds
  )
```

```{r}
nlm_wf %>% autoplot(metric = "rmse",select_best = TRUE)
nlm_wf %>% rank_results(rank_metric = "rmse")

lm_wf %>% 
  bind_rows(nlm_wf) %>% 
  autoplot(metric = "rmse", select_best = TRUE)
```

# Tree Models

### Random Forest
A random forest model, with mtry = 5, number of trees = 1000, and 20 random values of min_n

### Cubist Model
A Cubist Model tuned over the number of committees, neighbors, and rules. 50 evenly spaced random values were chosen for the grid.

### Boosted Tree
A boosted tree model was tuned over min n, tree depth, learn rate, loss reduction, and sample size. A grid of 50 evenly spaced values was chosen for the grid.

### Preprocess
Little preprocessing was done for the tree models. Dummy variables were made for the nominal predictors with one-hot encoding, and a near-zero variance filter for the predictor space. 
```{r}
tree_rec <- recipe(burn_rate~.,data = train) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_nzv(all_predictors())
```

```{r}
rforest_spec <- rand_forest(mode = "regression",
                            engine = "ranger",
                            mtry = 5,
                            trees = 1000,
                            min_n = tune())

cube_spec <- rules::cubist_rules(mode = "regression",
                                 committees = tune(),
                                 neighbors = tune(),
                                 max_rules = tune(),
                                 engine = "Cubist")

boost_spec <- boost_tree(mode = "regression",
                         engine = "xgboost",
                         mtry = 5,
                         trees = 1000,
                         min_n = tune(),
                         tree_depth = tune(),
                         learn_rate = tune(),
                         loss_reduction = tune(),
                         sample_size = tune())

tree_models <- list(rforest_spec,cube_spec,boost_spec)
```

```{r}
library(finetune)
tree_wf <- workflow_set(list(tree_rec), models = tree_models)

tree_wf <- workflow_map(tree_wf,
                        "tune_race_anova",
                        grid = 50,
                        resamples = folds,
                        verbose = TRUE,
                        metrics = metrics,
                        control = control_race(save_pred = TRUE,save_workflow = TRUE)
                        )
```

```{r}
tree_wf %>% autoplot()
```

# Training Model Results
We can see that neural network, cubist, and boosted tree models are all within standard errors of each other. We can do a repeated-measures ANOVA to check if there are any differences between the models or use a paired t-test.
```{r}
lm_wf %>% 
  bind_rows(nlm_wf,tree_wf) %>% 
  autoplot(metric = c("rmse","rsq","ccc"), select_best = TRUE)
```


# Results 
We get a test set RMSE of 0.0534458.
```{r}
nlm_wf %>% rank_results(rank_metric = "rmse")

nnet_wf <- nlm_wf %>% 
  extract_workflow("recipe_mlp")

nnet_wf_results <- nlm_wf %>% 
  extract_workflow_set_result("recipe_mlp")

best_nnet_res <- nnet_wf_results %>% 
  select_best("rmse")

nnet_wf_final <- finalize_workflow(nnet_wf,best_nnet_res)

test_res <- last_fit(nnet_wf_final,split = splits)

test_res %>% collect_metrics()
```

