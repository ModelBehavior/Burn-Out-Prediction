---
title: "Are your Employees Mental Health at Risk?"
subtitle: "Burn Out Prediction"
author: Ra'Shawn Howard
output: html_notebook
---

# context
Understanding what will be the Burn Rate for the employee working in an organization based on the current pandemic situation where work from home is a boon and a bane. How are employees' Burn Rate affected based on various conditions provided?

# Data
Globally, World Mental Health Day is celebrated on October 10 each year. The objective of this day is to raise awareness about mental health issues around the world and mobilize efforts in support of mental health. According to an anonymous survey, about 450 million people live with mental disorders that can be one of the primary causes of poor health and disability worldwide. These days when the world is suffering from a pandemic situation, it becomes really hard to maintain mental fitness. The Variables are: \

+ **Employee ID**: The unique ID allocated for each employee (example: fffe390032003000)
+ **Date of Joining**: The date-time when the employee has joined the organization (example: 2008-12-30)
+ **Gender**: The gender of the employee (Male/Female)
+ **Company Type**: The type of company where the employee is working (Service/Product)
+ **WFH Setup Available**: Is the work from home facility available for the employee (Yes/No)
+ **Designation**: The designation of the employee of work in the organization. In the range of [0.0, 5.0] bigger is higher designation.
+ **Resource Allocation**: The amount of resource allocated to the employee to work, ie. number of working hours. In the range of [1.0, 10.0] (higher means more resource)
+ **Mental Fatigue Score**: The level of fatigue mentally the employee is facing. In the range of [0.0, 10.0] where 0.0 means no fatigue and 10.0 means completely fatigue.
+ **Burn Rate**: The value we need to predict for each employee telling the rate of Bur out while working. In the range of [0.0, 1.0] where the higher the value is more is the burn out.

# Inspiration
Try to build some really amazing predictions keeping in mind that happy and healthy employees are indisputably more productive at work, and in turn, help the business flourish profoundly.


# Initail Data Analysis
Start by reading the data in and examining it:

```{r load-libraries-and-data, include=TRUE,warning=FALSE,echo=FALSE}
library(tidymodels)
library(tidyverse)

knitr::opts_chunk$set(echo=FALSE,include=FALSE,warning = FALSE)
theme_set(ggthemes::theme_hc())

burn_out <- read_csv(
  "/Users/rashawnhoward/Downloads/train.csv",
  col_names = TRUE
  ) %>% 
  drop_na() %>% 
  janitor::clean_names()

head(burn_out)
```

We can see that the column names will need to be reformatted.  We will get rid of the spaces in the names and the capital lettering. We can also see that gender, company type, and WHF setup available need to be re-coded as factors.

```{r summry-data, include=TRUE}
burn_out %>% 
  select(-employee_id) %>% 
  mutate(day = lubridate::day(date_of_joining),
         month = lubridate::month(date_of_joining),
         year = lubridate::year(date_of_joining),
         gender = as.factor(gender),
         company_type = as.factor(company_type),
         wfh_setup_available = as.factor(wfh_setup_available),
         designation = as.factor(designation),
         resource_allocation = as.factor(resource_allocation)) %>% 
  select(-date_of_joining) -> burn_out
```

We see that employee_id will not be useful in this analysis. We have about the same number of Females in the data set as males, but more females than males. For company type, we have almost double service compared to product. There are more work-from-home facilities than not, but the numbers are close.  We can also see that resoure_allocation, mental_fatigue_score, and burn_rate all have missing values. I'm not sure why these values are missing, and I have no one to ask why they are missing. These values will be removed before the analysis.

# Split Data
Before modeling, the data was split into a training and testing set, and a seed was set for reproducibility. 80% of the data were randomly allocated to the training set.
```{r}
# Initial split of the data into training and testing sets.
set.seed(8442)
splits <- initial_split(burn_out, prop = .8, strata = burn_rate)
train <- training(splits)
```


```{r resp-vs-predictors, include=TRUE,message=FALSE,warning=FALSE}
plot_lm <- function(data,predictor,response) { 
  data %>% 
  ggplot(aes({{predictor}},{{response}})) +
  geom_point() +
  geom_jitter() +
  geom_smooth(method = "lm")
} 

plot_cat <- function(data,predictor,response) {
  data %>% 
  ggplot(aes({{predictor}},{{response}})) +
  geom_violin() +
  geom_boxplot(aes(color = {{predictor}})) +
  theme(legend.position = "none")
}

g1 <- plot_cat(train,company_type,burn_rate)
g2 <- plot_lm(train,designation,burn_rate)
g3 <- plot_cat(train,gender,burn_rate)
g4 <- plot_lm(train,mental_fatigue_score,burn_rate)
g5 <- plot_lm(train,resource_allocation,burn_rate)
g6 <- plot_cat(train,wfh_setup_available,burn_rate)

ggpubr::ggarrange(g1,g2,g3,g4,g5,g6,
                  nrow = 2,ncol = 3)
```

Strong relationships can be seen in several of the plots. Resource allocation and designation seems like they can be encoded as a categorical variable. This is hard to see from the plots above, but individual plots of the variables show this. The plot of them individually can be seen below. In the plot for mental fatigue score, we can the fitted line goes below zero, which is problematic since the observed values of burn rate cannot be negative. This could be helped with a transfomation. Looking at the violin/boxplots, we can see that company type as pretty symmetrical distribution with respect to burn rate, as well as identical medians. Gender and wfh setup available appear to have skewed distribution with respect to burn rate and unequal medians. **NOTE: This is not a hypothesis test and could be different**.

```{r bin-plots, include=TRUE,message=FALSE,warning=FALSE}
g7 <- plot_lm(train,designation,burn_rate) 
g8 <- plot_lm(train,resource_allocation,burn_rate)

ggpubr::ggarrange(g7,g8,
                  nrow = 2,ncol = 1)
```

```{r predictor-vs-predictor, include=TRUE, message=FALSE, warning=FALSE}
train %>% 
  GGally::ggpairs(columns = c("designation","mental_fatigue_score","resource_allocation"),
                  uupper ='cor')
```

There is a high degree of correlation between designation, mental fatigue score, and resource allocation. I will be going with an alternate encoding of these variables and turning designation and resource allocation into categorical variables. The transformation of these two predictors can be seen below.

```{r}
g9 <- train %>% 
  mutate(designation = as.factor(designation),
         resource_allocation = as.factor(resource_allocation)) %>% 
  ggplot(aes(designation,burn_rate)) +
  geom_boxplot()

g10 <- train %>% 
  mutate(designation = as.factor(designation),
         resource_allocation = as.factor(resource_allocation)) %>% 
  ggplot(aes(resource_allocation,burn_rate)) +
  geom_boxplot()

ggpubr::ggarrange(g9,g10,
                  ncol = 1,nrow = 2)
```

# Metrics and Controls
The metrics used to investigate the models are RMSE, R-squared, CCC, and MAE.
```{r}
metrics <- metric_set(rmse,rsq,mae,yardstick::ccc)
ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
```

# Resamples
10-fold cross-validation is used to split the data for tuning and evaluation of training models.
```{r}
set.seed(2028)
folds <- vfold_cv(train, v = 10)
```

# Linear Models
OLS, ridge, LASSO, elastic net regression was used on the data. The tuning parameters for the penalty models was found using 10-fold cross-validation over a random grid of 20 values.
### PreProcess
The numeric predictors were transformed using Yeojohnson transformations, centered and scaled, dummy variables were made for the nominal predictors, near-zero
variance filter was done on the predictor's space, and a correlation filter was done on the predictors.
```{r}
lin_rec <- recipe(burn_rate~., data = train) %>% 
  step_rm(year,day,month) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = .70)
```

```{r}
lm_mod <- linear_reg(
  engine = "lm")

ridge_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = 0)

lasso_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = 1)

elastic_mod <- linear_reg(
  engine = "glmnet",
  penalty = tune(),
  mixture = tune())

linear_mods_list <- list(lm_mod,ridge_mod,lasso_mod,elastic_mod)
```

```{r}
lm_wf <- workflow_set(list(lin_rec),models = linear_mods_list)

lm_wf <- workflow_map(
  lm_wf,
  "tune_grid",
  grid = 20,
  verbose = TRUE,
  metrics = metrics,
  control = ctrl,
  resamples = folds
  )
```

```{r}
lm_wf %>% autoplot(metric = "rmse")
lm_wf %>% rank_results(rank_metric = "rmse")
```

# Non-Linear Models

### KNN
KNN was tuned over the number of neighbors (k) in the range 1-10 inclusive.

### MARS
MARS model was tuned over the degree of polynomial 1 or 2 and the number of terms to keep in the model.

### NNET
The single-layer neural network model tuned over the number of hidden units [1,10], dropout, weight decay, and 500 training iterations.

### SVM
A radial basis support vector machine tuned over cost, sigma, and margin with 20 randomly spaced values.

### Preprocess 
The preprocessing of the data for the non-linear models included centering and scaling numeric predictors, making dummy variables for all nominal predictors, and checking for near-zero variance predictors.
```{r}
nlin_rec <- recipe(burn_rate~.,data = burn_out) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())
```

```{r}
knn_spec <- nearest_neighbor( 
  mode = "regression",
  engine = "kknn",
  neighbors = tune()
  )

mars_spec <- mars(
  mode = "regression",
  engine = "earth",
  num_terms = tune(),
  prod_degree = tune()
  )
nnet_spec <- mlp(mode = "regression",
                 engine = "nnet",
                 hidden_units = tune(),
                 penalty = tune(),
                 dropout = tune(),
                 epochs = 500,
                 activation = "linear"
                 )
svm_spec <- svm_rbf(mode = "regression",
                    engine = "kernlab",
                    cost = tune(),
                    rbf_sigma = tune(),
                    margin = tune()
                    )
non_linear_models <- list(knn_spec,mars_spec,nnet_spec,svm_spec)
```

```{r}
nlm_wf <- workflow_set(list(nlin_rec),models = non_linear_models)

nlm_wf <- workflow_map(
  nlm_wf,
  "tune_grid",
  verbose = TRUE,
  grid = 20,
  metrics = metrics,
  control = ctrl,
  resamples = folds
  )
```

```{r}
nlm_wf %>% autoplot(metric = "rmse",select_best = TRUE)
nlm_wf %>% rank_results(rank_metric = "rmse")

lm_wf %>% 
  bind_rows(nlm_wf) %>% 
  autoplot(metric = "rmse", select_best = TRUE)
```

# Tree Models

### Random Forest
A random forest model, with mtry = 5, number of trees = 1000, and 20 random values of min_n

### Cubist Model
A Cubist Model tuned over the number of committees, neighbors, and rules. 50 evenly spaced random values were chosen for the grid.

### Boosted Tree
A boosted tree model was tuned over min n, tree depth, learn rate, loss reduction, and sample size. A grid of 50 evenly spaced values was chosen for the grid.

### Preprocess
Little preprocessing was done for the tree models. Dummy variables were made for the nominal predictors with one-hot encoding, and a near-zero variance filter for the predictor space. 
```{r}
tree_rec <- recipe(burn_rate~.,data = train) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_nzv(all_predictors())
```

```{r}
rforest_spec <- rand_forest(mode = "regression",
                            engine = "ranger",
                            mtry = 5,
                            trees = 1000,
                            min_n = tune())

cube_spec <- rules::cubist_rules(mode = "regression",
                                 committees = tune(),
                                 neighbors = tune(),
                                 max_rules = tune(),
                                 engine = "Cubist")

boost_spec <- boost_tree(mode = "regression",
                         engine = "xgboost",
                         mtry = 5,
                         trees = 1000,
                         min_n = tune(),
                         tree_depth = tune(),
                         learn_rate = tune(),
                         loss_reduction = tune(),
                         sample_size = tune())

tree_models <- list(rforest_spec,cube_spec,boost_spec)
```

```{r}
library(finetune)
tree_wf <- workflow_set(list(tree_rec), models = tree_models)

tree_wf <- workflow_map(tree_wf,
                        "tune_race_anova",
                        grid = 50,
                        resamples = folds,
                        verbose = TRUE,
                        metrics = metrics,
                        control = control_race(save_pred = TRUE,save_workflow = TRUE)
                        )
```

```{r}
tree_wf %>% autoplot()
```

# Training Model Results
We can see that neural network, cubist, and boosted tree models are all within standard errors of each other. We can do a repeated-measures ANOVA to check if there are any differences between the models or use a paired t-test.
```{r}
lm_wf %>% 
  bind_rows(nlm_wf,tree_wf) %>% 
  autoplot(metric = c("rmse","rsq","ccc"), select_best = TRUE)
```

# Results 
We get a test set RMSE of 0.0534458.
```{r}
nlm_wf %>% rank_results(rank_metric = "rmse")

nnet_wf <- nlm_wf %>% 
  extract_workflow("recipe_mlp")

nnet_wf_results <- nlm_wf %>% 
  extract_workflow_set_result("recipe_mlp")

best_nnet_res <- nnet_wf_results %>% 
  select_best("rmse")

nnet_wf_final <- finalize_workflow(nnet_wf,best_nnet_res)

test_res <- last_fit(nnet_wf_final,split = splits)

test_res %>% collect_metrics()
```

